{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- loading train data\n",
      "loding /input/train/c0, image count=1707\n",
      "loding /input/train/c1, image count=2113\n",
      "loding /input/train/c2, image count=868\n",
      "loding /input/train/c3, image count=1691\n",
      "loding /input/train/c4, image count=1805\n",
      "loding /input/train/c5, image count=1976\n",
      "loding /input/train/c6, image count=1728\n",
      "loding /input/train/c7, image count=1300\n",
      "loding /input/train/c8, image count=722\n",
      "loding /input/train/c9, image count=1596\n",
      "-------- loading valid data\n",
      "loding /input/valid/c0, image count=92\n",
      "loding /input/valid/c1, image count=158\n",
      "loding /input/valid/c2, image count=247\n",
      "loding /input/valid/c3, image count=345\n",
      "loding /input/valid/c4, image count=388\n",
      "loding /input/valid/c5, image count=364\n",
      "loding /input/valid/c6, image count=328\n",
      "loding /input/valid/c7, image count=298\n",
      "loding /input/valid/c8, image count=108\n",
      "loding /input/valid/c9, image count=316\n"
     ]
    }
   ],
   "source": [
    "basedir = \"/input/\"\n",
    "model_image_size = 224\n",
    "\n",
    "\n",
    "print(\"-------- loading train data\")\n",
    "X_train = list()\n",
    "y_train = list()\n",
    "for i in range(10):\n",
    "    dir = os.path.join(basedir, \"train\", \"c%d\"%i)\n",
    "    image_files = glob.glob(os.path.join(dir,\"*.jpg\")) #返回F:\\pose_recognication\\data\\train\\ci文件夹中所有以jpg结尾的文件的路径\n",
    "    print(\"loding {}, image count={}\".format(dir, len(image_files)))\n",
    "   #data_files=glob.glob(\"data/*.txt\")\n",
    "\n",
    "#         glob.glob()返回的是列表 list类型。是所有路径下的符合条件的文件名的列表。\n",
    "\n",
    "#        此例中参数为相对路径，指simple_httpd.py  web服务器当前目录下的data文件夹下的所有txt文件。\n",
    "\n",
    "#        也可以为绝对路径\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(image_file)\n",
    "        X_train.append(cv2.resize(image, (model_image_size, model_image_size)))\n",
    "        label = np.zeros(10, dtype=np.uint8)\n",
    "        label[i]=1\n",
    "        y_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "        \n",
    "print(\"-------- loading valid data\")\n",
    "X_valid = list()\n",
    "y_valid = list()\n",
    "for i in range(10):\n",
    "    dir = os.path.join(basedir, \"valid\", \"c%d\"%i)\n",
    "    image_files = glob.glob(os.path.join(dir,\"*.jpg\"))\n",
    "    print(\"loding {}, image count={}\".format(dir, len(image_files)))\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(image_file)\n",
    "        X_valid.append(cv2.resize(image, (model_image_size, model_image_size)))\n",
    "        label = np.zeros(10, dtype=np.uint8)\n",
    "        label[i]=1\n",
    "        y_valid.append(label)\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 132\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#base_model = VGG16(input_tensor=Input((model_image_size, model_image_size, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "#base_model = InceptionV3(input_tensor=Input((model_image_size, model_image_size, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "fine_tune_layer = 116\n",
    "final_layer = 135\n",
    "visual_layer = 132\n",
    "batch_size = 64\n",
    "base_model = Xception(input_tensor=Input((model_image_size, model_image_size, 3)), weights='imagenet', include_top=False)\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "model = Model(base_model.input, x)\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"total layer count {}\".format(len(base_model.layers)))\n",
    "\n",
    "for i in range(fine_tune_layer):\n",
    "    model.layers[i].trainable = False\n",
    "\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15506 samples, validate on 2644 samples\n",
      "Epoch 1/10\n",
      "15506/15506 [==============================] - 109s 7ms/step - loss: 0.0985 - acc: 0.9676 - val_loss: 2.8227 - val_acc: 0.8239\n",
      "Epoch 2/10\n",
      "15506/15506 [==============================] - 106s 7ms/step - loss: 0.0380 - acc: 0.9898 - val_loss: 2.7109 - val_acc: 0.8239\n",
      "Epoch 3/10\n",
      "15506/15506 [==============================] - 106s 7ms/step - loss: 0.0300 - acc: 0.9915 - val_loss: 2.8175 - val_acc: 0.8239\n",
      "Epoch 4/10\n",
      "15506/15506 [==============================] - 107s 7ms/step - loss: 0.0243 - acc: 0.9929 - val_loss: 2.8200 - val_acc: 0.8239\n",
      "Epoch 5/10\n",
      "15506/15506 [==============================] - 107s 7ms/step - loss: 0.0223 - acc: 0.9935 - val_loss: 2.8229 - val_acc: 0.8239\n",
      "Epoch 6/10\n",
      "15506/15506 [==============================] - 106s 7ms/step - loss: 0.0211 - acc: 0.9937 - val_loss: 2.8174 - val_acc: 0.8239\n",
      "Epoch 7/10\n",
      "15506/15506 [==============================] - 106s 7ms/step - loss: 0.0191 - acc: 0.9944 - val_loss: 2.8187 - val_acc: 0.8239\n",
      "Epoch 8/10\n",
      "15506/15506 [==============================] - 106s 7ms/step - loss: 0.0181 - acc: 0.9945 - val_loss: 2.8220 - val_acc: 0.8239\n",
      "Epoch 9/10\n",
      "15506/15506 [==============================] - 107s 7ms/step - loss: 0.0175 - acc: 0.9949 - val_loss: 2.8184 - val_acc: 0.8239\n",
      "Epoch 10/10\n",
      "15506/15506 [==============================] - 106s 7ms/step - loss: 0.0182 - acc: 0.9944 - val_loss: 2.8224 - val_acc: 0.8239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff81241fbe0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=4, epochs=10, validation_data=(X_valid, y_valid))\n",
    "#model.save(\"/data/inception-not-finetune.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/data/Xception-not-finetune.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
