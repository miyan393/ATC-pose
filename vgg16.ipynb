{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- loading train data\n",
      "loding /input/train/c0, image count=1707\n",
      "loding /input/train/c1, image count=2113\n",
      "loding /input/train/c2, image count=868\n",
      "loding /input/train/c3, image count=1691\n",
      "loding /input/train/c4, image count=1805\n",
      "loding /input/train/c5, image count=1976\n",
      "loding /input/train/c6, image count=1728\n",
      "loding /input/train/c7, image count=1300\n",
      "loding /input/train/c8, image count=722\n",
      "loding /input/train/c9, image count=1596\n",
      "-------- loading valid data\n",
      "loding /input/valid/c0, image count=92\n",
      "loding /input/valid/c1, image count=158\n",
      "loding /input/valid/c2, image count=247\n",
      "loding /input/valid/c3, image count=345\n",
      "loding /input/valid/c4, image count=388\n",
      "loding /input/valid/c5, image count=364\n",
      "loding /input/valid/c6, image count=328\n",
      "loding /input/valid/c7, image count=298\n",
      "loding /input/valid/c8, image count=108\n",
      "loding /input/valid/c9, image count=316\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#basedir = r'F:\\pose_recognication\\data'\n",
    "basedir = \"/input/\"\n",
    "model_image_size = 224\n",
    "\n",
    "print(\"-------- loading train data\")\n",
    "X_train = list()\n",
    "y_train = list()\n",
    "for i in range(10):\n",
    "    dir = os.path.join(basedir, \"train\", \"c%d\"%i)\n",
    "    image_files = glob.glob(os.path.join(dir,\"*.jpg\")) #返回F:\\pose_recognication\\data\\train\\ci文件夹中所有以jpg结尾的文件的路径\n",
    "    print(\"loding {}, image count={}\".format(dir, len(image_files)))\n",
    "   #data_files=glob.glob(\"data/*.txt\")\n",
    "\n",
    "#         glob.glob()返回的是列表 list类型。是所有路径下的符合条件的文件名的列表。\n",
    "\n",
    "#        此例中参数为相对路径，指simple_httpd.py  web服务器当前目录下的data文件夹下的所有txt文件。\n",
    "\n",
    "#        也可以为绝对路径\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(image_file)\n",
    "        X_train.append(cv2.resize(image, (model_image_size, model_image_size)))\n",
    "        label = np.zeros(10, dtype=np.uint8)\n",
    "        label[i]=1\n",
    "        y_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "        \n",
    "print(\"-------- loading valid data\")\n",
    "X_valid = list()\n",
    "y_valid = list()\n",
    "for i in range(10):\n",
    "    dir = os.path.join(basedir, \"valid\", \"c%d\"%i)\n",
    "    image_files = glob.glob(os.path.join(dir,\"*.jpg\"))\n",
    "    print(\"loding {}, image count={}\".format(dir, len(image_files)))\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(image_file)\n",
    "        X_valid.append(cv2.resize(image, (model_image_size, model_image_size)))\n",
    "        label = np.zeros(10, dtype=np.uint8)\n",
    "        label[i]=1\n",
    "        y_valid.append(label)\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15506, 224, 224, 3)\n",
      "(15506, 10)\n",
      "(2644, 224, 224, 3)\n",
      "(2644, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 96s 2us/step\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(input_tensor=Input((model_image_size, model_image_size, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "for layers in base_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "model = Model(base_model.input, x)\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "for i in range(fine_tune_layer):\n",
    "    model.layers[i].trainable = False\n",
    "\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15506 samples, validate on 2644 samples\n",
      "Epoch 1/10\n",
      "15506/15506 [==============================] - 108s 7ms/step - loss: 0.1174 - acc: 0.9718 - val_loss: 0.3440 - val_acc: 0.8868\n",
      "Epoch 2/10\n",
      "15506/15506 [==============================] - 103s 7ms/step - loss: 0.0068 - acc: 0.9981 - val_loss: 0.2753 - val_acc: 0.9183\n",
      "Epoch 3/10\n",
      "15506/15506 [==============================] - 103s 7ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.3104 - val_acc: 0.9034\n",
      "Epoch 4/10\n",
      "15506/15506 [==============================] - 103s 7ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.3547 - val_acc: 0.9029\n",
      "Epoch 5/10\n",
      "15506/15506 [==============================] - 104s 7ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.3506 - val_acc: 0.8996\n",
      "Epoch 6/10\n",
      "15506/15506 [==============================] - 103s 7ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.3214 - val_acc: 0.9149\n",
      "Epoch 7/10\n",
      "15506/15506 [==============================] - 103s 7ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.3493 - val_acc: 0.9055\n",
      "Epoch 8/10\n",
      "15506/15506 [==============================] - 103s 7ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.3194 - val_acc: 0.9114\n",
      "Epoch 9/10\n",
      "15506/15506 [==============================] - 103s 7ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.3319 - val_acc: 0.9172\n",
      "Epoch 10/10\n",
      "15504/15506 [============================>.] - ETA: 0s - loss: 6.7979e-04 - acc: 0.9997"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=4, epochs=10, validation_data=(X_valid, y_valid))\n",
    "\n",
    "#model.save(r\"F:\\pose_recognication\\vgg16-mymodel.h5\")\n",
    "model.save(\"/data/vgg16-mymodel-wangluo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
